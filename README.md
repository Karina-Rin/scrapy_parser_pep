[![Python](https://img.shields.io/badge/-Python-464646?style=flat&logo=Python&logoColor=ffff19&color=1446a3)](https://www.python.org/)
![Scrapy logo](https://tinyurl.com/y4d7aet3)

# Асинхронный парсер PEP

Парсер документов PEP на базе фреймворка Scrapy.

## Описание проекта

Парсер собирает информацию в два файла с ресурса https://www.python.org/:

- В первый файл выводится список всех PEP: номер, название и статус.
- Второй файл содержит сводку по статусам PEP — сколько найдено документов в 
каждом статусе (статус, количество). В последней строке этого файла в колонке 
«Статус» указывается слово Total, а в колонке «Количество» — 
общее количество всех документов.

Парсер сохраняет нформацию в файлы `.csv` в папке `results/`.

**Файлы со списком PEP** именованы по маске `pep_ДатаВремя.csv`, содержат три 
столбца: «Номер», «Название» и «Статус». Сохранение выполняется посредством 
*Feeds*.

**Файлы со сводкой по статусам** именованы по маске 
`status_summary_ДатаВремя.csv`, содержат два столбца: «Статус» и «Количество». 
Для создания этого файла описан pipeline, который суммирует количество 
документов PEP в разных статусах и по окончании парсинга формирует файл `.csv`.
Дополнительно в pipeline считайется общее количество документов PEP, в 
последней строке со сводкой в столбце «Статус» указано “Total”, а в столбце 
«Количество» выводится общее количество полученных документов PEP. Сохранение 
выполняется посредством через *Pipeline*.

### Работа с парсерами

1. Клонируем репозиторий:

```
git clone https://github.com/Karina-Rin/scrapy_parser_pep.git
```

2. Создаём и активируем виртуальное окружение:

```
python -m venv venv
source venv/Scripts/activate
```

3. Обновляем менеджер пакетов pip и устанавливаем зависимости:
```
pip install --upgrade --force-reinstall -r requirements.txt
pip install -r requirements.txt
```

4. Запускаем парсеры:

```
scrapy crawl pep
```

### Автор
- [Karina-Rin](https://github.com/Karina-Rin "GitHub аккаунт")
