![Scrapy logo](https://camo.githubusercontent.com/40d00cefb120a829517e503658aaf6c987d5f9cc6be5e2e35fb20bd63bdbceb5/68747470733a2f2f7363726170792e6f72672f696d672f7363726170796c6f676f2e706e67)


# Асинхронный парсер PEP

Парсер документов PEP на базе фреймворка Scrapy.

## Описание проекта

Парсер собирает информацию в два файла с ресурса https://www.python.org/:

- В первый файл выводится список всех PEP: номер, название и статус.
- Второй файл содержит сводку по статусам PEP — сколько найдено документов в 
каждом статусе (статус, количество). В последней строке этого файла в колонке 
«Статус» указывается слово Total, а в колонке «Количество» — 
общее количество всех документов.

Парсер сохраняет нформацию в файлы `.csv` в папке `results/`.

**Файлы со списком PEP** именованы по маске `pep_ДатаВремя.csv`, содержат три 
столбца: «Номер», «Название» и «Статус». Сохранение выполняется посредством 
*Feeds*.

**Файлы со сводкой по статусам** именованы по маске 
`status_summary_ДатаВремя.csv`, содержат два столбца: «Статус» и «Количество». 
Для создания этого файла описан pipeline, который суммирует количество 
документов PEP в разных статусах и по окончании парсинга формирует файл `.csv`.
Дополнительно в pipeline считайется общее количество документов PEP, в 
последней строке со сводкой в столбце «Статус» указано “Total”, а в столбце 
«Количество» выводится общее количество полученных документов PEP. Сохранение 
выполняется посредством через *Pipeline*.

### Работа с парсерами

1. Клонируем репозиторий:

```
git clone https://github.com/Karina-Rin/scrapy_parser_pep.git
```

2. Создаём и активируем виртуальное окружение:

```
python -m venv venv
source venv/Scripts/activate
```

3. Обновляем менеджер пакетов pip и устанавливаем зависимости:
```
pip install --upgrade --force-reinstall -r requirements.txt
pip install -r requirements.txt
```

4. Запускаем парсеры:

```
scrapy crawl pep
```

### Автор
- [Karina-Rin](https://github.com/Karina-Rin "GitHub аккаунт")
